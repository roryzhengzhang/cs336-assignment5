import torch
from typing import Any


def log_generations(
    prompts: list[str],
    responses: list[str],
    ground_truths: list[str],
    rewards: list[dict[str, float]],
    token_entropies: list[torch.Tensor] | None = None,
    response_masks: list[torch.Tensor] | None = None,
) -> dict[str, Any]:
    """
    Log generation information for model evaluation.

    Args:
        prompts: List of input prompts.
        responses: List of responses generated by the SFT/RL model.
        ground_truths: List of ground-truth answers.
        rewards: List of reward dictionaries, each containing:
            - "format_reward": float (0.0 or 1.0)
            - "answer_reward": float (0.0 or 1.0)
            - "reward": float (total reward, 0.0 or 1.0)
        token_entropies: Optional list of per-token entropy tensors (batch_size, sequence_length).
        response_masks: Optional list of response masks (batch_size, sequence_length) to compute response lengths.

    Returns:
        dict containing:
            - "examples": List of dicts with individual generation info
            - "avg_format_reward": Average format reward across all examples
            - "avg_answer_reward": Average answer reward across all examples
            - "avg_total_reward": Average total reward across all examples
            - "avg_token_entropy": Average token entropy across all responses (if provided)
            - "avg_response_length": Average response length across all examples (if masks provided)
            - "avg_correct_response_length": Average response length for correct responses (if masks provided)
            - "avg_incorrect_response_length": Average response length for incorrect responses (if masks provided)
            - "accuracy": Proportion of correct responses (reward == 1.0)
    """
    num_examples = len(prompts)
    assert len(responses) == num_examples
    assert len(ground_truths) == num_examples
    assert len(rewards) == num_examples

    # Individual example logs
    examples = []
    for i in range(num_examples):
        example = {
            "prompt": prompts[i],
            "response": responses[i],
            "ground_truth": ground_truths[i],
            "format_reward": rewards[i]["format_reward"],
            "answer_reward": rewards[i]["answer_reward"],
            "total_reward": rewards[i]["reward"],
        }

        # Add token entropy if provided
        if token_entropies is not None:
            # Compute average entropy for this response
            entropy = token_entropies[i]  # (sequence_length,) or (sequence_length, vocab_size)
            if response_masks is not None:
                # Only average over response tokens
                mask = response_masks[i]
                masked_entropy = entropy * mask
                avg_entropy = masked_entropy.sum() / mask.sum()
            else:
                avg_entropy = entropy.mean()
            example["avg_token_entropy"] = avg_entropy.item()

        # Add response length if masks provided
        if response_masks is not None:
            response_length = response_masks[i].sum().item()
            example["response_length"] = response_length

        examples.append(example)

    # Aggregate statistics
    total_format_reward = sum(r["format_reward"] for r in rewards)
    total_answer_reward = sum(r["answer_reward"] for r in rewards)
    total_reward = sum(r["reward"] for r in rewards)

    stats = {
        "examples": examples,
        "avg_format_reward": total_format_reward / num_examples,
        "avg_answer_reward": total_answer_reward / num_examples,
        "avg_total_reward": total_reward / num_examples,
        "accuracy": total_reward / num_examples,  # Since reward is 0 or 1
    }

    # Compute average token entropy across all responses
    if token_entropies is not None:
        if response_masks is not None:
            # Average only over response tokens across all examples
            total_entropy = 0.0
            total_tokens = 0
            for i in range(num_examples):
                entropy = token_entropies[i]
                mask = response_masks[i]
                total_entropy += (entropy * mask).sum().item()
                total_tokens += mask.sum().item()
            stats["avg_token_entropy"] = total_entropy / total_tokens if total_tokens > 0 else 0.0
        else:
            # Simple average across all tokens
            all_entropies = torch.stack([e.mean() for e in token_entropies])
            stats["avg_token_entropy"] = all_entropies.mean().item()

    # Compute response length statistics
    if response_masks is not None:
        response_lengths = [mask.sum().item() for mask in response_masks]
        stats["avg_response_length"] = sum(response_lengths) / num_examples

        # Separate by correctness
        correct_lengths = [
            response_lengths[i]
            for i in range(num_examples)
            if rewards[i]["reward"] == 1.0
        ]
        incorrect_lengths = [
            response_lengths[i]
            for i in range(num_examples)
            if rewards[i]["reward"] == 0.0
        ]

        stats["avg_correct_response_length"] = (
            sum(correct_lengths) / len(correct_lengths) if correct_lengths else 0.0
        )
        stats["avg_incorrect_response_length"] = (
            sum(incorrect_lengths) / len(incorrect_lengths) if incorrect_lengths else 0.0
        )
        stats["num_correct"] = len(correct_lengths)
        stats["num_incorrect"] = len(incorrect_lengths)

    return stats
